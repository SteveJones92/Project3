[
  {
    "objectID": "pages/EDA.html#exploratory-data-analysis",
    "href": "pages/EDA.html#exploratory-data-analysis",
    "title": "EDA",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nIntroduction\nThe dataset we will be using Data Link comes from another dataset with over 300 features Base Data Link. It has been cleaned into a much smaller set of predictors (mostly binary) for use in building a model to predict diabetes or no diabetes.\nA quick search of diabetes influencing factors shows Weight, Inactivity, Family history, Age, Race and ethnicity, Prediabetes, Gestational diabetes, Blood pressure, Cholesterol. Of which, we have access to columns that contain:\n\nHigh Blood Pressure\nHigh Cholesterol\nDifficulty walking (similar to inactivity)\nAge\n\nAnd many other fields (not exhaustive):\n\nSmoker (5+ packs in lifetime)\nGeneral Health Rating (1-5, lowest being best)\nStroke (Ever had)\nFruits (1+ a day)\n\nWe will first explore the data and then process the data into what is needed for building a classification tree and random forest model.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# load data\ndata &lt;- read.csv(\"../data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n\n\nCheck missing values\n\nsum(is.na(data))\n\n[1] 0\n\n\n\nThere are no missing values in the data, which makes sense because the dataset has already been processed.\n\n\n\nDiabetes proportion\n\nn &lt;- nrow(data) # 253680 items\nn\ntable(data$Diabetes_binary) / n\n\n[1] 253680\n\n       0        1 \n0.860667 0.139333 \n\n\n\nThere are 253680 rows in the data. Out of this, it is seen that 14% of the data are diabetes=true and 86% do not have diabetes. This means that if the guess is just to put no diabetes for everything, the accuracy would be 86%, and any model should do better than this.\n\n\n\nExploring Yes/No Categories (14 out of 21)\n\nhelper &lt;- function(category) {\n  # finds the proportions of the category for yes/no categories\n  # finds the proportion of each combination\n  # finds the percentages of diabetes based on the category\n  prop &lt;- table(data[[category]]) / n\n  ct &lt;- table(ifelse(data$Diabetes_binary == 1, \"Diabetes\", \"NoDiabetes\" ),\n              ifelse(data[[category]] == 1, category, paste0(\"No\", category))) / n\n  explained &lt;- t(t(ct) / ( ct[1,] + ct[2,]))\n  max(explained[,1]) * prop[2] + max(explained[,2]) * prop[1]\n  print(prop)\n  print(ct)\n  print(explained)\n}\n\n\nAs seen in further use, this will give a good view of each field and how much influence it may have on the response.\n\n\nFrequent Items\n\nhelper(\"HighBP\")\n\n\n        0         1 \n0.5709989 0.4290011 \n            \n                 HighBP   NoHighBP\n  Diabetes   0.10487228 0.03446074\n  NoDiabetes 0.32412882 0.53653816\n            \n                 HighBP   NoHighBP\n  Diabetes   0.24445690 0.06035167\n  NoDiabetes 0.75554310 0.93964833\n\n\n\nThe split of the data on high blood pressure vs not high blood pressure is 57% and 42%, respectively. We can see of the 57% for 0 HighBP, 53.6% is no diabetes and 3.4% is diabetes. The proportion here is 93.9% and 6%, meaning if we flat-out guessed no diabetes for this NoHighBP items, we’d get a 93.9% accuracy. But we want to learn about the more rare class of diabetes. So a good thing to look at is the ratio between NoHighBP and HighBP. If we had diabetes, we’d be about 4x more likely to have HighBP. Other items in this category are HighChol, PhysActivity, Smoker, and DiffWalk.\n\n\n\nInfrequent Items\n\nhelper(\"CholCheck\")\n\n\n        0         1 \n0.0373305 0.9626695 \n            \n                CholCheck  NoCholCheck\n  Diabetes   0.1383830022 0.0009500158\n  NoDiabetes 0.8242865027 0.0363804793\n            \n              CholCheck NoCholCheck\n  Diabetes   0.14374923  0.02544879\n  NoDiabetes 0.85625077  0.97455121\n\n\n\nThere are a set of items here where most of the rows are 1 value. Almost all respondents (96%) has had a cholesterol check within the last 5 years. So even if we got info from this, it wouldn’t apply in most cases. That said, if you did get info, the ratio is large (~7x). Other items in this category are Stroke, HeartDiseaseorAttack, and HvyAlcoholConsump.\n\n\n\nLittle to no information\n\nhelper(\"Fruits\")\nhelper(\"Sex\")\n\n\n        0         1 \n0.3657442 0.6342558 \n            \n                 Fruits   NoFruits\n  Diabetes   0.08157127 0.05776175\n  NoDiabetes 0.55268448 0.30798250\n            \n                Fruits  NoFruits\n  Diabetes   0.1286094 0.1579293\n  NoDiabetes 0.8713906 0.8420707\n\n        0         1 \n0.5596578 0.4403422 \n            \n                  NoSex        Sex\n  Diabetes   0.07257569 0.06675733\n  NoDiabetes 0.48708215 0.37358483\n            \n                 NoSex       Sex\n  Diabetes   0.1296787 0.1516033\n  NoDiabetes 0.8703213 0.8483967\n\n\n\nThese items don’t really have a spread of diabetes vs non-diabetes. Basically, if you knew someone was male or female, it wouldn’t tell you anything more about diabetes or not by itself. The same for Fruits, Veggies, AnyHealthcare, and NoDocbcCost. Now it’s possible that these proportions change in conjunction with another set of information (maybe fruits + high bp would have a better separation and fruit has typically high sugar content). It may be worth it to include in a tree build just for 1, to see if it makes a difference.\n\n\n\n\nExploring non-yes/no items of BMI, GenHlth, MentHlth, PhysHlth, Age, Education, Income\n\n# leftover BMI, GenHlth, MentHlth, PhysHlth, Age, Education, Income\npaste0(cor(data$MentHlth, data$Diabetes_binary), \" MentHlth\")\npaste0(cor(data$PhysHlth, data$Diabetes_binary), \" PhysHlth\")\npaste0(cor(data$GenHlth, data$Diabetes_binary), \" GenHlth\")\npaste0(cor(data$BMI, data$Diabetes_binary), \" BMI\")\npaste0(cor(data$Age, data$Diabetes_binary), \" Age\")\npaste0(cor(data$Education, data$Diabetes_binary), \" Education\")\npaste0(cor(data$Income, data$Diabetes_binary), \" Income\")\n\n[1] \"0.0693150826381003 MentHlth\"\n[1] \"0.171336700387463 PhysHlth\"\n[1] \"0.29356906307932 GenHlth\"\n[1] \"0.216843060203153 BMI\"\n[1] \"0.177441872167362 Age\"\n[1] \"-0.124455969216236 Education\"\n[1] \"-0.163918786800468 Income\"\n\n\n\nThe highest correlation is General Health (1 being best and 5 being worst). So higher age, thinking about physical or mental health, generally worse health ratings, and higher BMI have positive correlations (leading towards diabetes). Higher education and higher income lead towards not having diabetes. Let’s look at some plots.\n\n\nggplot(data, aes(x = Age, fill = factor(Diabetes_binary))) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(data, aes(x = Income, fill = factor(Diabetes_binary))) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(data, aes(x = Education, fill = factor(Diabetes_binary))) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(data, aes(x = GenHlth, fill = factor(Diabetes_binary))) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(data, aes(x = MentHlth, fill = factor(Diabetes_binary))) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(data, aes(x = PhysHlth, fill = factor(Diabetes_binary))) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(data, aes(x = BMI, fill = factor(Diabetes_binary))) +\n    geom_histogram(position = \"fill\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nhist(data$BMI, breaks = 20)\n\n\n\n\n\n\n\n\n\nAge, GenHlth, and BMI are generally supported as having an effect on the response. BMI is quite rare after values of 40, and with diabetes being a generally rare class, it makes sense for the chart to lose shape at the higher values. PhysHlth and MentHlth are not the best and likely represented a good amount through GenHlth reporting, so let’s just use that. As for education and income, they look viable to use, but don’t make a lot of sense for explainability of outcome. These are likely represented by way of effect of income on MentHlth for example (and many other categories, like needing to see a doctor but couldn’t past 12 month). These will intentionally not be used.\n\n\n\nBundling frequent and infrequent items (general idea of presenting with more items is more likely to suggest for diabetes). This is common in the health industry.\n\n# summing across the common items that have an effect\n# summing across the rare items that have an effect\ndata_fixed &lt;- data |&gt;\n  mutate(\n    count_common = rowSums(across(c(HighBP, HighChol, Smoker, PhysActivity, DiffWalk))),\n    count_rare = rowSums(across(c(CholCheck, Stroke, HeartDiseaseorAttack, HvyAlcoholConsump)))\n  ) |&gt;\n  select(-c(Fruits, Veggies, AnyHealthcare, NoDocbcCost, Income, Education, PhysHlth, MentHlth))\n\ntable(db = data_fixed$Diabetes_binary, common = data_fixed$count_common)\ntable(db = data_fixed$Diabetes_binary, rare = data_fixed$count_rare)\n\n   common\ndb      0     1     2     3     4     5\n  0  8094 63874 72014 49193 21853  3306\n  1   409  3039  8275 12248  9193  2182\n   rare\ndb       0      1      2      3      4\n  0   8159 177971  29100   3019     85\n  1    174  25047   8363   1738     24\n\n\n\nShowing the diabetes counts across the tallies of these common and rare items, we can see the ratios get smaller as more items are tacked on. It is typical practice in healthcare to have patients that present with higher number of red-flag factors to be more susceptible to other worse outcomes.\n\n\n\nConverting column structures to factors and numbers.\n\ndata_fixed_m &lt;- data_fixed |&gt;\n  mutate(\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\")),\n    Age = factor(Age, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80+\")),\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n  )\nstr(data_fixed_m)\n\n'data.frame':   253680 obs. of  16 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"Excellent\",\"Very Good\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ DiffWalk            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : num  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : Factor w/ 13 levels \"18-24\",\"25-29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ count_common        : num  4 2 3 2 3 4 2 5 4 0 ...\n $ count_rare          : num  1 0 1 1 1 1 1 1 2 1 ...\n\n\n\nAlmost all of the data are factors. Only BMI and the counts created are non-factors. Convert all to appropriate labels. Our data should be ready for sending to models for usage."
  },
  {
    "objectID": "pages/Modeling.html#building-2-models-predicting-diabetes_binary-using-log-loss-as-evaluation-metric",
    "href": "pages/Modeling.html#building-2-models-predicting-diabetes_binary-using-log-loss-as-evaluation-metric",
    "title": "Models",
    "section": "Building 2 Models predicting ‘Diabetes_binary’ using ‘log-loss’ as evaluation metric",
    "text": "Building 2 Models predicting ‘Diabetes_binary’ using ‘log-loss’ as evaluation metric\n\nIntroduction\nThe data resulting from the previous EDA will be used in creating the predictive models for diabetes. We will perform a comparison between a tuned classification tree and a tuned random forest model. The chosen model is of the form:\n\ny = BMI + Age + GenHlth + count_common + count_rare\n\nWhere count_common is a count how many of the common influencing factors are present (HighBP, HighChol) and count_rare is less frequent items (Stroke).\n\n\nlibrary(tidymodels)\nlibrary(parsnip)\n\n\n\nData split\n\nset.seed(107)\ndata_split &lt;- initial_split(data_fixed_m, prop = 0.7, strata = Diabetes_binary)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\ndata_5_fold &lt;- vfold_cv(data_train, v=5, strata = Diabetes_binary)\n\n\nFirst we split the data into train and test set. This allows us to get a more accurate sense of how the model will perform on unseen data. The folds used for cross-validation in tuning model parameters is set up as well. Using 5-fold due to run-time of the random-forest.\n\n\n\nModel recipe\n\nmodel_recipe &lt;- recipe(Diabetes_binary ~ BMI + Age + GenHlth + count_common + count_rare, data = data_train) |&gt;\n  step_mutate(\n    GenHlth = factor(GenHlth, ordered = TRUE),\n    Age = factor(Age, ordered = TRUE)\n  )\n\nmodel_recipe |&gt; prep() |&gt; bake(new_data = data_train)\n\n# A tibble: 177,575 × 6\n     BMI Age   GenHlth   count_common count_rare Diabetes_binary\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;          \n 1    40 60-64 Poor                 4          1 No             \n 2    25 50-54 Good                 2          0 No             \n 3    28 60-64 Poor                 3          1 No             \n 4    27 70-74 Very Good            2          1 No             \n 5    24 70-74 Very Good            3          1 No             \n 6    30 60-64 Good                 2          1 No             \n 7    25 70-74 Good                 5          1 No             \n 8    24 55-59 Very Good            0          1 No             \n 9    34 65-69 Good                 4          1 No             \n10    26 50-54 Good                 1          1 No             \n# ℹ 177,565 more rows\n\n\n\nThe model recipe predictors are those from the prior processing, leaving out the factors that make up the count_common and count_rare fields. Using the set of those over the counts did not seem to improve prediction, so this keeps the complexity down. GenHlth and Age are specified as ordered factors for the splits to be chosen by a split and not by dummy variables.\n\n\n\nModel 1: Classification Tree\nA classification tree will try to build a tree that splits the data on different values of the predictors. It tries to find the best splits, or the ones that separate the most “pure” part of the data to one side and the more mixed data on the other. Pure meaning all or most of the example data had one outcome on the response variable, we are saying that if we see this value, then we think it’s likely a new data point would also have that outcome.\nThe classification tree has some hyperparameters that will effect the splits and tree size/complexity.\n\nTree depth: How deep can the tree expand to, or how many successive splits are allowed?\nMin split: How many values are needed to split on. If how many is in a particular branch of the tree is 25 items of 70/30 mix, but we had set 30 min to split, then it would split no further (even though there is more separation to be had).\nCost complexity: Is there a penalization for having more complex trees? So it would need to gain enough information to justify the split, given the penalizing factor.\n\n\n\nclass_tree_spec &lt;- decision_tree(tree_depth = tune(), min_n = 20, cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode (\"classification\")\n\nclass_tree_wkf &lt;- workflow() |&gt;\n  add_recipe(model_recipe) |&gt;\n  add_model(class_tree_spec)\n\n\nThe tuning will be done on 2 parameters, tree_depth and cost_complexity. These values are in ranges empirically chosen from stepping up the data size and getting multiple smaller tunes (for speed purposes), including the choice of min_n. The value of min_n is set to 20 (searching through 5-60) by results of the smaller tuning results. The tree_depth and cost_complexity will further do a larger tuning search on the full training set.\n\n\nclass_tree_grid &lt;- class_tree_wkf |&gt;\n  tune_grid(\n    resamples=data_5_fold,\n    grid=grid_regular(\n        tree_depth(range(6L, 20L)),\n        cost_complexity(range(-10, -1)),\n      levels=c(5, 15)),\n    metrics = metric_set(mn_log_loss, accuracy)\n #   control = control_grid(verbose = TRUE)\n  )\n\n\nThe tree depth in most runs tends to be around 11. The cost complexity always comes out to be the minimum of anything given, as in the most complex tree is always favored. More focus on the tuning will be done on the cost_complexity range.\n\n\nclass_tree_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  ggplot(aes(x=tree_depth, y=mean, color=as.factor(cost_complexity))) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nThese plots show the log_loss value result from the various tuning settings.\n\n\nbest_log_loss &lt;- class_tree_grid |&gt;\n    select_best(metric = \"mn_log_loss\")\nbest_log_loss\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          9 Preprocessor1_Model02\n\n\n\nUsing log_loss, select the combination that resulted in the best (lowest) log_loss value. This should be the best version (or close to) of this model for this metric that can be achieved.\n\n\nclass_tree_final_fit &lt;- class_tree_wkf |&gt;\n    finalize_workflow(best_log_loss) |&gt;\n    last_fit(data_split, metrics = metric_set(mn_log_loss, accuracy))\n\n\nFinally, we fit the model on the full data.\n\n\ntree_final_model &lt;- extract_workflow(class_tree_final_fit)\ntree_final_model |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\n\n\n\nThis is the resulting tree, which is likely very complex given the tree_depth and cost_complexity tuning. Primarily because the cost_complexity seems to not favor penalizing at all.\n\n\nclass_tree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.863 Preprocessor1_Model1\n2 mn_log_loss binary         0.340 Preprocessor1_Model1\n\n\n\nThe resulting log_loss and accuracy are shown here. It is minimized for log_loss, but accuracy is presented just as a check against the 0.86 or better claim for just predicting all as no diabetes.\n\n\n\nModel 2: Random Forest\nA random forest is like the basic classification tree, but very many simpler trees are constructed slightly differently and the full amalgamation of these trees vote on what is the final prediction. For each tree, it randomly selects a group of predictors to use.\nThe random forest has two hyperparameters:\n\nNumber of sampled predictors: How many predictors to use for each of the small trees when randomly selecting.\nNumber of trees: How many trees are constructed totally for the model.\nMin split: How many values are needed to split on (same as classification tree).\n\n\n\nRF_spec &lt;- rand_forest(\n  mtry = tune(),  # default is 1\n  trees = tune(),  # default is 500\n  min_n = tune()) |&gt;  # default is 2\n  set_engine(\"ranger\", importance=\"impurity\") |&gt;\n  set_mode(\"classification\")\n\nRF_wkf &lt;- workflow() |&gt;\n  add_recipe(model_recipe) |&gt;\n  add_model(RF_spec)\n\n\nTune on each of the values. Random forest takes a long time, so the ranges used will be empirically selected from smaller runs.\n\n\nRF_grid &lt;- RF_wkf |&gt;\n  tune_grid(\n    resamples = data_5_fold,\n    grid = grid_regular(\n      mtry(range(1L, 2L)),\n      trees(range(1000L, 3000L)),\n      min_n(range(2L, 40L)),\n      levels=c(2, 3, 4)\n    ),\n    metrics = metric_set(mn_log_loss, accuracy)\n #   control = control_grid(verbose = TRUE)\n  )\n\n\nMtry always comes out as 1 in every iteration. To keep the cost of tuning down, we will only consider 1 against 2 for this value. The number of trees and the min split values will be considered in further detail.\n\n\nRF_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  ggplot(aes(x=mtry, y=mean, color=as.factor(trees))) +\n  facet_wrap(~min_n) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nThis plot shows all of the tuned values, where log_loss minimum is desired.\n\n\nbest_log_loss &lt;- RF_grid |&gt;\n    select_best(metric = \"mn_log_loss\")\nbest_log_loss\n\n# A tibble: 1 × 4\n   mtry trees min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     2  3000    40 Preprocessor1_Model24\n\n\n\nSelect for the best combination of the 3 hyperparameters in terms of log_loss.\n\n\nRF_final_fit &lt;- RF_wkf |&gt;\n    finalize_workflow(best_log_loss) |&gt;\n    last_fit(data_split, metrics = metric_set(mn_log_loss, accuracy))\n\n\nFit the final random forest model using these values.\n\n\nRF_final_model &lt;- extract_fit_engine(RF_final_fit)\nimportance_values &lt;- RF_final_model$variable.importance\nimportance_df &lt;- data.frame(\n  term = names(importance_values),\n  value = importance_values\n) |&gt;\n  as_tibble() |&gt;\n  arrange(desc(value))\n\nimportance_df |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x=term, y=value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nFurther, we can get variable importance (could do for the other), just to get a sense of what is directing the model more substantially in it’s predictions.\n\n\nRF_final_fit |&gt;\n    collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.864 Preprocessor1_Model1\n2 mn_log_loss binary         0.324 Preprocessor1_Model1\n\n\nThe outcome metrics for this best model. Log loss is minimized and accuracy is present as well, as a comparison against the “everything no diabetes” guess.\n\nEDA Page"
  },
  {
    "objectID": "pages/EDA.html#to-modeling",
    "href": "pages/EDA.html#to-modeling",
    "title": "EDA",
    "section": "To Modeling",
    "text": "To Modeling"
  }
]